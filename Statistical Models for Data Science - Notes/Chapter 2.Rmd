---
title: "Chapter 2 â€” Useful Approximations"
output: html_document
---
# Newtonâ€“Raphson method

## â˜€ï¸ Motivation

In Chapter 1, we learned to find maximum likelihood estimates (MLEs) using calculus:
we took the derivative of a log-likelihood function, set it equal to zero, and solved for the parameter.  
However, not all likelihood equations have **closed-form solutions**.  
When this happens, we use **numerical optimization methods** â€” and one of the most famous is the **Newtonâ€“Raphson method**.

\[
\frac{d\ell(\theta)}{d\theta} = 0
\]

If this equation canâ€™t be solved algebraically, we can *approximate* the solution with an iterative algorithm.

---

## âš™ï¸ The Newtonâ€“Raphson Algorithm

The Newtonâ€“Raphson method is used to find the **root of an equation**, that is, the value of \( x \) for which \( f(x) = 0 \).

Starting from an initial guess \( x_0 \), we update it iteratively using:

\[
x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}
\]

This process continues until the updates become very small (i.e., convergence).

In statistical applications, we apply this to the **derivative of the log-likelihood**:

\[
\ell'(\theta) = 0
\]

and use Newtonâ€“Raphson to approximate the value of \( \theta \) that satisfies it.

\[
\theta_{new} = \theta_{old} - \frac{\ell'(\theta_{old})}{\ell''(\theta_{old})}
\]

---

## ğŸŒ¿ Conceptual Summary

| Step | Description |
|------|--------------|
| 1ï¸âƒ£ | Start with an initial guess \( \theta_0 \) |
| 2ï¸âƒ£ | Compute the first derivative \( \ell'(\theta_0) \) |
| 3ï¸âƒ£ | Compute the second derivative \( \ell''(\theta_0) \) |
| 4ï¸âƒ£ | Update: \( \theta_{new} = \theta_{old} - \ell' / \ell'' \) |
| 5ï¸âƒ£ | Repeat until change is small (convergence) |

---

## ğŸ’¡ Connection to Likelihood

In Chapter 1, we maximized likelihoods analytically for simple distributions:

| Distribution | Parameter(s) | Closed-form MLE? | Notes |
|---------------|--------------|------------------|-------|
| Bernoulli / Binomial | \( p \) | âœ… Yes | \( \hat{p} = \bar{x} \) |
| Poisson | \( \lambda \) | âœ… Yes | \( \hat{\lambda} = \bar{x} \) |
| Normal | \( \mu, \sigma^2 \) | âœ… Yes | \( \hat{\mu} = \bar{x}, \hat{\sigma}^2 = \frac{1}{n}\sum(x_i - \bar{x})^2 \) |
| Exponential | \( \lambda \) | âœ… Yes | \( \hat{\lambda} = n / \sum x_i \) |
| Logistic Regression | \( \beta_0, \beta_1, \ldots \) | âŒ No | Requires iterative methods |
| Mixture Models | multiple | âŒ No | Often solved with EM or Newton-type methods |

For simple cases (like Binomial, Poisson, Normal, Exponential), the derivative equals zero can be solved directly.  
But for **non-closed-form models** such as **logistic regression** or **mixture models**, we use **Newtonâ€“Raphson** to approximate where \( \ell'(\theta) = 0 \).

---

## ğŸ¨ Intuitive Visualization

Newtonâ€“Raphson uses the **tangent line** at the current point \( x_n \) to approximate where the function crosses zero.

The secant method is related â€” it replaces the tangent slope with a **secant slope between two points** when derivatives are unavailable.

| Method | Line Type | Requires Derivative? |
|:--|:--|:--|
| Newtonâ€“Raphson | Tangent line | âœ… Yes |
| Secant method | Secant line (between two points) | âŒ No |

So both aim to find where \( f(x) = 0 \), but Newtonâ€“Raphson does so using the *true derivative*, while the Secant method estimates it numerically.

``` {r}
# Set up plotting area
par(mfrow = c(1, 2), mar = c(4, 3, 4, 2) + 0.1, bg = "white")

# Define function
fsec <- function(x) 2 * log(x) - x + 3
sec.x <- seq(0.5, 2.5, 0.05)

## Left: Exact derivative using tangent line
plot(sec.x, fsec(sec.x), type = "l", lwd = 2.5, col = "#66CDAA",
     ylim = c(1, 2.8), main = "Exact derivative of a function",
     xlab = "x", ylab = "f(x)", cex.main = 1.1)
# Add tangent line (tangent slope = 1/3)
abline(fsec(1.5) - (1/3)*1.5, 1/3, col = "#FF69B4", lwd = 2)
# Add vertical reference
lines(x = c(1.5, 1.5), y = c(0, fsec(1.5)), lty = 2, col = "gray50")
# Add annotation
text(0.85, 2.4, expression("f'(1.5) = 0.333..."), col = "#FF69B4", cex = 0.9)
points(1.5, fsec(1.5), pch = 19, col = "#FF69B4")

## Right: Approximation using secant line
plot(sec.x, fsec(sec.x), type = "l", lwd = 2.5, col = "#66CDAA",
     ylim = c(1, 2.8), main = "Approximation using secant line",
     xlab = "x", ylab = "f(x)", cex.main = 1.1)
# Îµ (step size)
eps <- 0.2
# Points for secant
x.left <- 1.5 - eps
x.right <- 1.5 + eps
# Draw dashed guide lines
lines(x = c(x.left, x.left), y = c(0, fsec(x.left)), lty = 3, col = "gray60")
lines(x = c(x.right, x.right), y = c(0, fsec(x.right)), lty = 3, col = "gray60")
# Secant slope and line
sec.slope <- (fsec(x.right) - fsec(x.left)) / (x.right - x.left)
abline(fsec(x.left) - sec.slope * x.left, sec.slope, col = "#FF69B4", lwd = 2)
# Highlight points
points(c(x.left, x.right), c(fsec(x.left), fsec(x.right)), pch = 19, col = "#FF69B4")
# Text labels
text(0.85, 2.4, expression("f'(1.5)" %~~% 0.341), col = "#FF69B4", cex = 0.9)
text(1.5, 1.05, expression(x), cex = 0.9)
text(1.25, 1.05, expression(x - epsilon), cex = 0.9)
text(1.75, 1.05, expression(x + epsilon), cex = 0.9)
mtext("Visual comparison of tangent vs. secant slope", outer = TRUE, line = -1.5, cex = 0.9, col = "gray40")

```

---

## ğŸŒ¼ Summary Real-World Use Case Example Table

| Scenario | Example Task | Best Method | Why |
|:--|:--|:--|:--|
| Logistic Regression | Predict binary outcomes | Newtonâ€“Raphson | Has derivatives, fast convergence |
| Distribution Fitting | Estimate Gamma / Weibull parameters | Newtonâ€“Raphson | Nonlinear MLE |
| Neural Networks | Optimize loss function | Newtonâ€“Raphson-type (BFGS/Adam) | Gradient-based |
| Simulation / Black Box | Find target value with unknown slope | Secant Method | Derivative-free |
| ARIMA / GARCH Models | Fit time series parameters | Newtonâ€“Raphson | Smooth, non-closed form likelihood |

---

## ğŸ§  Summary Points

- MLE finds the parameter value that makes the observed data **most likely**.  
- Solving \( \ell'(\theta) = 0 \) gives the MLE.  
- If no algebraic solution exists, we use **Newtonâ€“Raphson** (iterative method).  
- It uses derivatives:  
  - \( \ell'(\theta) \): slope (first derivative)  
  - \( \ell''(\theta) \): curvature (second derivative)  
- Works best when the log-likelihood is smooth and concave.  
- For hard-to-differentiate models, other root-finding methods (like the Secant method) can approximate similar results.

---

âœ¨ *In short:*  
> Newtonâ€“Raphson is how we let computers â€œdo calculus for usâ€ when likelihood equations are too messy to solve by hand.

## âš™ï¸ Programing Example

```{r}
#coin data generated with true p=0.6
set.seed(2061)
coins <- sample(0:1,50,TRUE,c(0.4,0.6))

# derivative of log-likelihood for coin parameter
coin.dll <- function(p) sum(coins)/p-sum(1-coins)/(1-p)

# helper function to approximate derivatives
ddx <- function(value, func, delta=1e-6){
  (func(value+delta)-func(value-delta))/(2*delta)}

# core newton-raphson algorithm, plus some output
newton <- function(f, x0, tol=1e-6, max.iter=20){
  x <- x0
  i <- 0
  results <- data.frame(iter=i,x=x,fx=f(x))
  while (abs(f(x))>tol & i<max.iter) {
    i <- i + 1
    x <- x - f(x)/ddx(x,f)
    results <- rbind(results,data.frame(iter=i,x=x,fx=f(x)))}
  return(results)}
```

# ğŸŒˆ Gradient Descent

## â˜€ï¸ Motivation
In the **Newtonâ€“Raphson method**, we found roots of equations using both the first and second derivatives.  
That approach works for simple, one-variable problems, but it has several drawbacks:

- âŒ Hard to extend to **functions with multiple parameters** (e.g., the Normal distribution with both mean \( \mu \) and variance \( \sigma^2 \))  
- âŒ Requires **second derivatives (Hessians)**, which are complex or expensive to compute  
- âŒ Can be **inefficient**, taking large jumps and overshooting the target  

To address these issues, we use **Gradient Descent** â€” a simpler, more general optimization method that scales to large models and high dimensions.

---

## ğŸŒ¿ Concept and Intuition
Imagine you are standing on a **3D landscape** â€” a hill or valley representing your functionâ€™s shape.  
Each coordinate (\( \theta_1, \theta_2, ... \)) is a parameter in your model.

- The **gradient** tells you which direction is steepest uphill.  
- Moving **against** the gradient takes you downhill (toward the minimum).  
- Taking **small steps** repeatedly leads you to a local optimum.

The update rule is:

\[
\theta_{t+1} = \theta_t - \alpha \nabla f(\theta_t)
\]

where  
\( \alpha \) = step size (learning rate)  
\( \nabla f(\theta_t) \) = gradient (vector of first derivatives)

If we wanted to maximize the function, we would use:

\[
\theta_{t+1} = \theta_t + \alpha \nabla f(\theta_t)
\]

This is called **gradient ascent**, but in practice we usually minimize the **negative log-likelihood**, so the algorithm is known as **gradient descent**.

---

## ğŸ’« Why â€œDescentâ€?
In statistics, we typically want to **maximize** the likelihood function \( \ell(\theta) \),  
but computers find it easier to **minimize** its negative counterpart:

\[
\max_\theta \ell(\theta) \quad \Leftrightarrow \quad \min_\theta [-\ell(\theta)]
\]

So even though conceptually weâ€™re climbing â€œuphillâ€ toward the MLE, in code weâ€™re minimizing a function â€” hence the term *gradient descent*.

---

## ğŸ§­ The Gradient Vector
For a function with multiple parameters \( f(\theta_1, \theta_2, ..., \theta_p) \),  
the gradient is a **vector of partial derivatives**:

\[
\nabla f(\theta) =
\begin{bmatrix}
\frac{\partial f}{\partial \theta_1} \\
\frac{\partial f}{\partial \theta_2} \\
\vdots \\
\frac{\partial f}{\partial \theta_p}
\end{bmatrix}
\]

It gives both the **direction** and **rate** of steepest increase.

So while Newtonâ€“Raphson uses tangent **lines** in one dimension,  
gradient descent uses tangent **planes** in higher dimensions.

---

## ğŸŒ¸ Step Size (Learning Rate)
The **step size** \( \alpha \) (or learning rate) controls how far you move each iteration:

- ğŸš€ Too large â†’ overshoot or diverge  
- ğŸ¢ Too small â†’ slow convergence  
- ğŸŒ¿ Just right â†’ smooth and stable descent

Choosing the right \( \alpha \) is crucial for convergence and efficiency.

---

## ğŸ€ Backtracking Line Search
To make step sizes adaptive, we use a **backtracking line search**.  

**Idea:**
1. Start with a large step size (t = 1).  
2. If the new step makes the function worse, shrink it: \( t = t \times b \), where \( b < 1 \) (e.g., 0.8).  
3. Repeat until improvement occurs, then accept that step.

This allows the algorithm to take **big steps when safe** and **small steps when necessary**.

---

### ğŸ’» Example: Estimating Î¼ and ÏƒÂ² for Normal Data

Hereâ€™s a demonstration using gradient descent to estimate the **mean** and **variance**  
of normally distributed height data, with a **backtracking line search**.

```{r}
# Gradient descent with backtracking line search
graddesc <- function(f, g, x0, b = 0.8, tol = 1e-2, max.iter = 40){
  i <- 0
  x <- x0
  results <- t(c(0, x, -f(x)))
  while (sqrt(g(x) %*% g(x)) > tol & i < max.iter) {
    i <- i + 1
    t <- 1
    while ((f(x + t * g(x)) < f(x)) & t > tol) {
      t <- t * b
    }
    x <- x + t * g(x)
    results <- rbind(results, t(c(i, x, -f(x))))
  }
  colnames(results) <- c('iter', paste0('x', 1:length(x)), 'f')
  return(results)
}

# Height data
heights <- c(170.1,161.6,175.2,166.3,165.6,165.8,152.0,155.8,168.6,154.3)

# Log-likelihood and gradient functions
height.ll <- function(x){
  -1 * length(heights) * log(2 * pi * x[2]) / 2 -
    sum((heights - x[1])^2) / (2 * x[2])
}

height.grad <- function(x){
  c(sum(heights - x[1]) / x[2],
    -1 * length(heights) / (2 * x[2]) +
      sum((heights - x[1])^2) / (2 * x[2]^2))
}

# Apply gradient descent
gd <- graddesc(f = height.ll, g = height.grad, x0 = c(150, 40))
gd[c(1:5, 41),]
```

## ğŸŒˆ The Central Limit Theorem (CLT)

### â˜€ï¸ The Sample Mean as a Random Variable
Every time we draw a sample from a population, the **sample mean** (\( \bar{X} \)) changes slightly.  
That means \( \bar{X} \) itself is a **random variable** with its own distribution.  

However, the distribution of the sample mean is *not* the same as the populationâ€™s distribution:
- If the population is **Uniform**, the sample mean is not uniform.  
- If the population is **Poisson**, the sample variance is not Poisson.  
- Even if the population is **skewed**, the sample meanâ€™s distribution will smooth out.

---

### ğŸŒ¿ What the CLT Says
The **Central Limit Theorem (CLT)** tells us how sample means behave when sample size increases.

Formally:

> Let \( X_1, X_2, ..., X_n \) be independent random variables from a population with  
> mean \( \mu \) and finite variance \( \sigma^2 \).  
> Then as \( n \to \infty \),
> \[
> Z = \frac{\bar{X} - \mu}{\sigma / \sqrt{n}} \to N(0,1)
> \]

That means:
\[
\bar{X} \sim N\left(\mu, \frac{\sigma^2}{n}\right)
\]

So, as the sample size grows:
- The **shape** of the sampling distribution of the mean becomes **Normal**,  
- The **spread** (Standard Error) becomes **smaller**: \( SE = \frac{\sigma}{\sqrt{n}} \).

---

### ğŸ’« What This Means in Practice
Even if your population data are not Normal, the **sample means** will be approximately Normal when \( n \) is large.  
This allows us to:
- Estimate **confidence intervals**,  
- Perform **hypothesis tests**, and  
- Use **Normal-based methods** safely, even for non-Normal data.

---

### ğŸ“‰ The Standard Error (SE)
The **CLT** tells us the shape of the sampling distribution, and the **Standard Error** gives its width:
\[
SE = \frac{\sigma}{\sqrt{n}}
\]

- The SE measures **how far the sample mean typically falls from the true mean**.  
- As \( n \) increases, the SE shrinks, meaning sample means become more stable.

| Concept | What it Describes | Formula | Purpose |
|:--|:--|:--|:--|
| **CLT** | Shape of sampling distribution of \( \bar{X} \) | \( N(\mu, \sigma^2/n) \) | Justifies using the Normal distribution |
| **SE** | Spread (standard deviation) of that distribution | \( \sigma / \sqrt{n} \) | Quantifies average distance from true mean |

---

## ğŸŒ· Example â€” Heights of Ten Women

Letâ€™s assume the same ten height measurements (cm) as before:

```{r}
heights <- c(170.1,161.6,175.2,166.3,165.6,165.8,152.0,155.8,168.6,154.3)
n <- length(heights)
sample_mean <- mean(heights)
pop_var <- 50
se <- sqrt(pop_var) / sqrt(n)

ci_lower <- sample_mean - 1.96 * se
ci_upper <- sample_mean + 1.96 * se
c("95% CI for mean height (cm)" = paste0(round(ci_lower,2), " to ", round(ci_upper,2)))
```

# ğŸŒˆ The Studentâ€™s t-Distribution

## â˜€ï¸ Motivation
The **Central Limit Theorem (CLT)** lets us approximate the sampling distribution of the sample mean using a Normal curve â€”  
but it assumes we already know the true population variance \( \sigma^2 \).

In practice, we almost never know \( \sigma^2 \).  
We must **estimate** it from the sample using \( s^2 \), which introduces extra uncertainty.  
This changes the behavior of our standardized statistic:

\[
\frac{\bar{X} - \mu}{s / \sqrt{n}}
\]

It no longer follows the Normal curve.  
Instead, it follows a slightly wider curve with thicker tails: the **Studentâ€™s t-distribution.**

---

```{r}
x <- seq(-4, 4, 0.01)
plot(x, dnorm(x), type = "l", lwd = 2, col = "#66CDAA",
     main = "Normal vs Student's t Distributions", ylab = "Density")
lines(x, dt(x, df = 5), col = "#F78FB3", lwd = 2)
lines(x, dt(x, df = 30), col = "#E75480", lwd = 2)
legend("topright", legend = c("Normal (z)", "t, df=5", "t, df=30"),
       col = c("#66CDAA", "#F78FB3", "#E75480"), lwd = 2, cex = 0.8)
```

---

## ğŸŒ¿ The Problem in Words
When Ïƒ is known (theoretical case):

\[
Z = \frac{\bar{X} - \mu}{\sigma / \sqrt{n}} \sim N(0,1)
\]

When Ïƒ is *unknown* (real-world case):

\[
T = \frac{\bar{X} - \mu}{s / \sqrt{n}} \sim t_{n-1}
\]

The difference?  
We replaced the fixed population SD \( \sigma \) with the random sample SD \( s \).  
Because \( s \) varies across samples, the resulting distribution â€œspreads outâ€ more, especially for small n.  
The t-distribution corrects for this.

---

## ğŸ“˜ Variable Reference

| Symbol | Name | What It Represents |
|:--|:--|:--|
| \( X_i \) | Observation | A single data point in the sample |
| \( n \) | Sample size | Number of observations |
| \( \mu \) | True population mean | The real (unknown) mean of the population |
| \( \bar{X} \) | Sample mean | Average of the sample values |
| \( \sigma, \sigma^2 \) | True population SD / variance | Fixed but unknown spread of the population |
| \( s, s^2 \) | Sample SD / variance | Estimates of \( \sigma \) and \( \sigma^2 \) |
| \( SE \) | Standard Error | \( s / \sqrt{n} \); typical distance between sample and true mean |
| \( df \) | Degrees of freedom | \( n - 1 \); adjusts for estimating Ïƒ |
| \( Z \) | Z-statistic | Standardized value when Ïƒ known |
| \( T \) | T-statistic | Standardized value when Ïƒ unknown (uses s) |
| \( t_{n-1} \) | Studentâ€™s t-distribution | Probability model that \( T \) follows when Ïƒ unknown |
| \( t_{\alpha/2, n-1} \) | Critical t-value | Cutoff from t-distribution for desired confidence level |
| \( z_{\alpha/2} \) | Critical z-value | Cutoff from Normal distribution when Ïƒ known |
| \( \alpha \) | Significance level | e.g. 0.05 for 95% confidence |
| \( 1 - \alpha \) | Confidence level | Probability that interval contains Î¼ |

---

## ğŸ’¡ Extended Formula Reference (for CLT and t-tests)

| Formula | Meaning of Symbols |
|:--|:--|
| **CLT (Ïƒ known)**  \( Z = \frac{\bar{X} - \mu}{\sigma / \sqrt{n}} \) |  \( \bar{X} \): sample mean Â· \( \mu \): true mean Â· \( \sigma \): population SD Â· \( n \): sample size Â· \( Z \): standardized value |
| **t-statistic (Ïƒ unknown)**  \( T = \frac{\bar{X} - \mu}{s / \sqrt{n}} \) |  Same as above but uses \( s \) instead of \( \sigma \); follows t-distribution with \( df = n - 1 \) |
| **Confidence interval (t-based)**  \( \bar{X} \pm t_{\alpha/2, n-1}\frac{s}{\sqrt{n}} \) |  \( t_{\alpha/2, n-1} \): critical t-value Â· \( s/\sqrt{n} \): SE Â· bounds likely to contain Î¼ |
| **Pooled variance (equal-var two-sample)**  \( s_p^2 = \frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2} \) |  Combines both groupsâ€™ variances; \( s_1, s_2 \): SDs Â· \( n_1, n_2 \): sample sizes |
| **Two-sample t-test (equal var)**  \( t = \frac{\bar{X}_1 - \bar{X}_2}{s_p\sqrt{1/n_1 + 1/n_2}} \) |  \( \bar{X}_1, \bar{X}_2 \): means Â· \( s_p \): pooled SD Â· df = \( n_1 + n_2 - 2 \) |
| **Welch (unequal var)**  \( t = \frac{\bar{X}_1 - \bar{X}_2}{\sqrt{s_1^2/n_1 + s_2^2/n_2}} \) |  Uses group variances directly; df â‰ˆ \( \nu \) (Welchâ€“Satterthwaite formula) |
| **One-sample proportion test**  \( z = \frac{\hat{p} - p_0}{\sqrt{p_0(1 - p_0)/n}} \) |  \( \hat{p} \): sample proportion Â· \( p_0 \): hypothesized proportion |

---

## ğŸŒ¿ The t-Adjustment to the CLT
With an unknown variance, the CLT becomes:

\[
T = \frac{\bar{X} - \mu}{s / \sqrt{n}} \sim t_{n-1}
\]

As \( n \) grows, the t-distribution approaches the standard Normal.  
For small \( n \), its heavier tails account for the extra uncertainty in estimating \( \sigma \).

---

## ğŸ’« Confidence Intervals

| Case | Formula | Description |
|:--|:--|:--|
| **Ïƒ known (z-based)** | \( \bar{X} \pm z_{\alpha/2}\frac{\sigma}{\sqrt{n}} \) | Uses Normal distribution |
| **Ïƒ unknown (t-based)** | \( \bar{X} \pm t_{\alpha/2, n-1}\frac{s}{\sqrt{n}} \) | Uses t-distribution (adjusts for estimated Ïƒ) |

---

## ğŸ’» Example: One-Sample t-Test for Mean Height
```{r}
heights <- c(170.1,161.6,175.2,166.3,165.6,165.8,152.0,155.8,168.6,154.3)
n <- length(heights)
mean_h <- mean(heights)
sd_h <- sd(heights)
se_h <- sd_h / sqrt(n)

alpha <- 0.05
t_crit <- qt(1 - alpha/2, df = n - 1)
ci_lower <- mean_h - t_crit * se_h
ci_upper <- mean_h + t_crit * se_h

c("Sample mean" = round(mean_h, 2),
  "Sample SD" = round(sd_h, 2),
  "Standard Error" = round(se_h, 2),
  "95% CI Lower" = round(ci_lower, 2),
  "95% CI Upper" = round(ci_upper, 2))
```
## ğŸŒ Real-World Use Cases of the t-Distribution

The **Studentâ€™s t-distribution** appears constantly in applied data science, analytics, and experimental research â€”  
anytime we need to make inferences about a population mean without knowing the true variance.

Below are several examples showing when to use the t-distribution, what type of test applies,  
and *why* it is the correct choice.

---

## ğŸŒ¿ Common Scenarios

| Scenario | Example | Appropriate Method | Why Use t-Distribution |
|:--|:--|:--|:--|
| ğŸ§ª **Clinical trials** | Compare the mean blood pressure of a drug group vs. placebo | **Two-sample t-test** | Population variance unknown; small \( n \); data assumed Normal |
| âš™ï¸ **Quality control** | Compare mean product weights across two machines | **Pooled two-sample t-test** | Variances assumed equal; combines both groupsâ€™ variability |
| ğŸ’» **Machine learning model validation** | Compare average model accuracies across folds | **Welchâ€™s t-test** | Variances differ between folds; Ïƒ unknown |
| ğŸ“ˆ **A/B testing** | Compare mean click-through rates of two webpage designs | **Two-sample t-test or Welchâ€™s test** | Proportion data approximated as Normal via CLT; Ïƒ estimated |
| ğŸ›ï¸ **Marketing research** | Estimate the mean customer satisfaction score | **One-sample t-test** | Ïƒ unknown; single sample of ratings |
| ğŸ§¬ **Biological experiments** | Test average enzyme activity between treated and control groups | **Two-sample t-test** | Small sample, normally distributed measurements |
| ğŸ­ **Industrial processes** | Check if machine output matches a target mean | **One-sample t-test** | Compares sample mean to desired target value |
| ğŸ’¡ **Survey analysis** | Estimate mean opinion score from respondents | **t-based confidence interval** | Ïƒ unknown; use \( s \) to estimate uncertainty |

---

## ğŸŒ¸ Quick Guidelines

1. **Use the Normal (z) distribution** only if:
   - You *know* the population variance \( \sigma^2 \) (very rare), or  
   - Your sample is extremely large (\( n > 100 \)) so that \( s \) closely approximates \( \sigma \).

2. **Use the t-distribution** when:
   - The variance \( \sigma^2 \) is *unknown* (the usual case).  
   - The sample size is *small to moderate* (e.g., \( n < 30\!-\!50 \)).  
   - You are estimating or testing **means** (not proportions).

3. **Use Welchâ€™s test** instead of the pooled version if:
   - The two samples have noticeably *different variances* or *unequal sample sizes.*

---

## âœ¨ Takeaway

> The **t-distribution** underpins most real-world inferential work.  
> It bridges the gap between small, uncertain samples and valid statistical inference â€”  
> letting us test hypotheses and build confidence intervals even when Ïƒ is unknown.

---

# ğŸŒŸ Handy Rules of Thumb

## â˜€ï¸ Overview
These are practical mental shortcuts that allow data scientists to:
- Approximate confidence intervals and sample sizes on the fly,  
- Sanity-check outputs,  
- Or give quick answers when detailed computation isnâ€™t feasible.

Theyâ€™re not rigorous â€” but theyâ€™re surprisingly accurate for quick reasoning.

---

## ğŸŒ¿ Chebychevâ€™s Inequality

\[
P(|X - \mu| < k\sigma) \geq 1 - \frac{1}{k^2}
\]

| Distance from Mean (kÏƒ) | % Within Range (min.) | Example |
|:--|:--|:--|
| 1Ïƒ | â‰¥ 0% | No guarantee |
| 2Ïƒ | â‰¥ 75% | At least 3/4 of data within 2 SD |
| 3Ïƒ | â‰¥ 88.9% | At least 8/9 of data within 3 SD |
| 5Ïƒ | â‰¥ 96% | Very concentrated distributions |

âœ… Works for *any* distribution with finite variance.  
âš ï¸ Conservative â€” true Normal data are usually tighter.

---

## ğŸŒ¸ Common Normal Quantile Rule (68â€“95â€“99.7)

| Range | Approx. % | Description |
|:--|:--|:--|
| Â±1Ïƒ | 68% | â€œ1-sigmaâ€ rule |
| Â±2Ïƒ | 95% | â€œ2-sigmaâ€ rule |
| Â±3Ïƒ | 99.7% | â€œ3-sigmaâ€ rule |

> Many real-world distributions (large n Binomial, Poisson, t, Ï‡Â²) approximately follow this pattern.

---

## ğŸ’« Quick & Dirty Intervals and Tests

**Rule:**  
Under the null, a sample statistic will be within Â±2 standard errors of the mean about 95% of the time.

Use this for:
- Rough hypothesis checks
- Quick margin-of-error or sample-size estimates

---

## ğŸ§® Example 1: Sample Size for Proportions

To estimate a proportion \( p \) within Â±0.05 with 95% confidence:

\[
2 \times \sqrt{\frac{p(1-p)}{n}} = 0.05
\]

At max variance \( p = 0.5 \):
\[
n = 400
\]

| Desired Margin of Error | Approx. n |
|:--|:--|
| Â±0.10 | 100 |
| Â±0.05 | 400 |
| Â±0.02 | 2500 |

---

### ğŸ“ˆ Example 2: Poisson CI
If Î» = 100 births / 4 years â†’ mean = 25 per year.

\[
SE = \sqrt{Î» / years} = \sqrt{25} = 5
\]
95% CI â‰ˆ \( 25 \pm 2(5) = [15, 35] \)

---

## ğŸ§¾ Example 3: Ï‡Â² Quick Test
If Ï‡Â² = 118 with df = 72:
\[
E(X) = 72,\ \ SD = \sqrt{2\times72}=12
\]
Expected 95% upper â‰ˆ 72 + 2Ã—12 = 96 â†’ 118 > 96 â†’ Reject Hâ‚€.

---

## ğŸŒ¼ The Rule of Three

If you observe **zero** events out of *n* trials:

\[
p_{upper,95\%} \approx \frac{3}{n}
\]

| Observed Events | Sample Size (n) | 95% Upper Bound |
|:--|:--|:--|
| 0 | 100 | 3% |
| 0 | 300 | 1% |
| 0 | 1000 | 0.3% |

ğŸ’¡ Example:  
Inspect 300 products, find 0 defects â†’ defect rate < 1% with 95% confidence.

---

## ğŸŒ· Takeaway

> These rules arenâ€™t exact, but they give you a powerful sense of scale:  
> - Chebychevâ€™s Inequality â€” works for any data.  
> - 68â€“95â€“99.7 Rule â€” quick Normal approximation.  
> - Â±2Ã—SE â€” mental margin-of-error rule.  
> - Rule of Three â€” rapid upper bound for zero events.

They let you *think statistically* even when you donâ€™t have code or software in front of you.
