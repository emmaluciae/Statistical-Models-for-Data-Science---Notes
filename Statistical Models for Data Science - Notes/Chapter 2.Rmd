---
title: "Chapter 2 — Useful Approximations"
output: html_document
---
# Newton–Raphson method

## ☀️ Motivation

In Chapter 1, we learned to find maximum likelihood estimates (MLEs) using calculus:
we took the derivative of a log-likelihood function, set it equal to zero, and solved for the parameter.  
However, not all likelihood equations have **closed-form solutions**.  
When this happens, we use **numerical optimization methods** — and one of the most famous is the **Newton–Raphson method**.

\[
\frac{d\ell(\theta)}{d\theta} = 0
\]

If this equation can’t be solved algebraically, we can *approximate* the solution with an iterative algorithm.

---

## ⚙️ The Newton–Raphson Algorithm

The Newton–Raphson method is used to find the **root of an equation**, that is, the value of \( x \) for which \( f(x) = 0 \).

Starting from an initial guess \( x_0 \), we update it iteratively using:

\[
x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}
\]

This process continues until the updates become very small (i.e., convergence).

In statistical applications, we apply this to the **derivative of the log-likelihood**:

\[
\ell'(\theta) = 0
\]

and use Newton–Raphson to approximate the value of \( \theta \) that satisfies it.

\[
\theta_{new} = \theta_{old} - \frac{\ell'(\theta_{old})}{\ell''(\theta_{old})}
\]

---

## 🌿 Conceptual Summary

| Step | Description |
|------|--------------|
| 1️⃣ | Start with an initial guess \( \theta_0 \) |
| 2️⃣ | Compute the first derivative \( \ell'(\theta_0) \) |
| 3️⃣ | Compute the second derivative \( \ell''(\theta_0) \) |
| 4️⃣ | Update: \( \theta_{new} = \theta_{old} - \ell' / \ell'' \) |
| 5️⃣ | Repeat until change is small (convergence) |

---

## 💡 Connection to Likelihood

In Chapter 1, we maximized likelihoods analytically for simple distributions:

| Distribution | Parameter(s) | Closed-form MLE? | Notes |
|---------------|--------------|------------------|-------|
| Bernoulli / Binomial | \( p \) | ✅ Yes | \( \hat{p} = \bar{x} \) |
| Poisson | \( \lambda \) | ✅ Yes | \( \hat{\lambda} = \bar{x} \) |
| Normal | \( \mu, \sigma^2 \) | ✅ Yes | \( \hat{\mu} = \bar{x}, \hat{\sigma}^2 = \frac{1}{n}\sum(x_i - \bar{x})^2 \) |
| Exponential | \( \lambda \) | ✅ Yes | \( \hat{\lambda} = n / \sum x_i \) |
| Logistic Regression | \( \beta_0, \beta_1, \ldots \) | ❌ No | Requires iterative methods |
| Mixture Models | multiple | ❌ No | Often solved with EM or Newton-type methods |

For simple cases (like Binomial, Poisson, Normal, Exponential), the derivative equals zero can be solved directly.  
But for **non-closed-form models** such as **logistic regression** or **mixture models**, we use **Newton–Raphson** to approximate where \( \ell'(\theta) = 0 \).

---

## 🎨 Intuitive Visualization

Newton–Raphson uses the **tangent line** at the current point \( x_n \) to approximate where the function crosses zero.

The secant method is related — it replaces the tangent slope with a **secant slope between two points** when derivatives are unavailable.

| Method | Line Type | Requires Derivative? |
|:--|:--|:--|
| Newton–Raphson | Tangent line | ✅ Yes |
| Secant method | Secant line (between two points) | ❌ No |

So both aim to find where \( f(x) = 0 \), but Newton–Raphson does so using the *true derivative*, while the Secant method estimates it numerically.

``` {r}
# Set up plotting area
par(mfrow = c(1, 2), mar = c(4, 3, 4, 2) + 0.1, bg = "white")

# Define function
fsec <- function(x) 2 * log(x) - x + 3
sec.x <- seq(0.5, 2.5, 0.05)

## Left: Exact derivative using tangent line
plot(sec.x, fsec(sec.x), type = "l", lwd = 2.5, col = "#66CDAA",
     ylim = c(1, 2.8), main = "Exact derivative of a function",
     xlab = "x", ylab = "f(x)", cex.main = 1.1)
# Add tangent line (tangent slope = 1/3)
abline(fsec(1.5) - (1/3)*1.5, 1/3, col = "#FF69B4", lwd = 2)
# Add vertical reference
lines(x = c(1.5, 1.5), y = c(0, fsec(1.5)), lty = 2, col = "gray50")
# Add annotation
text(0.85, 2.4, expression("f'(1.5) = 0.333..."), col = "#FF69B4", cex = 0.9)
points(1.5, fsec(1.5), pch = 19, col = "#FF69B4")

## Right: Approximation using secant line
plot(sec.x, fsec(sec.x), type = "l", lwd = 2.5, col = "#66CDAA",
     ylim = c(1, 2.8), main = "Approximation using secant line",
     xlab = "x", ylab = "f(x)", cex.main = 1.1)
# ε (step size)
eps <- 0.2
# Points for secant
x.left <- 1.5 - eps
x.right <- 1.5 + eps
# Draw dashed guide lines
lines(x = c(x.left, x.left), y = c(0, fsec(x.left)), lty = 3, col = "gray60")
lines(x = c(x.right, x.right), y = c(0, fsec(x.right)), lty = 3, col = "gray60")
# Secant slope and line
sec.slope <- (fsec(x.right) - fsec(x.left)) / (x.right - x.left)
abline(fsec(x.left) - sec.slope * x.left, sec.slope, col = "#FF69B4", lwd = 2)
# Highlight points
points(c(x.left, x.right), c(fsec(x.left), fsec(x.right)), pch = 19, col = "#FF69B4")
# Text labels
text(0.85, 2.4, expression("f'(1.5)" %~~% 0.341), col = "#FF69B4", cex = 0.9)
text(1.5, 1.05, expression(x), cex = 0.9)
text(1.25, 1.05, expression(x - epsilon), cex = 0.9)
text(1.75, 1.05, expression(x + epsilon), cex = 0.9)
mtext("Visual comparison of tangent vs. secant slope", outer = TRUE, line = -1.5, cex = 0.9, col = "gray40")

```

---

## 🌼 Summary Real-World Use Case Example Table

| Scenario | Example Task | Best Method | Why |
|:--|:--|:--|:--|
| Logistic Regression | Predict binary outcomes | Newton–Raphson | Has derivatives, fast convergence |
| Distribution Fitting | Estimate Gamma / Weibull parameters | Newton–Raphson | Nonlinear MLE |
| Neural Networks | Optimize loss function | Newton–Raphson-type (BFGS/Adam) | Gradient-based |
| Simulation / Black Box | Find target value with unknown slope | Secant Method | Derivative-free |
| ARIMA / GARCH Models | Fit time series parameters | Newton–Raphson | Smooth, non-closed form likelihood |

---

## 🧠 Summary Points

- MLE finds the parameter value that makes the observed data **most likely**.  
- Solving \( \ell'(\theta) = 0 \) gives the MLE.  
- If no algebraic solution exists, we use **Newton–Raphson** (iterative method).  
- It uses derivatives:  
  - \( \ell'(\theta) \): slope (first derivative)  
  - \( \ell''(\theta) \): curvature (second derivative)  
- Works best when the log-likelihood is smooth and concave.  
- For hard-to-differentiate models, other root-finding methods (like the Secant method) can approximate similar results.

---

✨ *In short:*  
> Newton–Raphson is how we let computers “do calculus for us” when likelihood equations are too messy to solve by hand.

## ⚙️ Programing Example

```{r}
#coin data generated with true p=0.6
set.seed(2061)
coins <- sample(0:1,50,TRUE,c(0.4,0.6))

# derivative of log-likelihood for coin parameter
coin.dll <- function(p) sum(coins)/p-sum(1-coins)/(1-p)

# helper function to approximate derivatives
ddx <- function(value, func, delta=1e-6){
  (func(value+delta)-func(value-delta))/(2*delta)}

# core newton-raphson algorithm, plus some output
newton <- function(f, x0, tol=1e-6, max.iter=20){
  x <- x0
  i <- 0
  results <- data.frame(iter=i,x=x,fx=f(x))
  while (abs(f(x))>tol & i<max.iter) {
    i <- i + 1
    x <- x - f(x)/ddx(x,f)
    results <- rbind(results,data.frame(iter=i,x=x,fx=f(x)))}
  return(results)}
```

# 🌈 Gradient Descent

## ☀️ Motivation
In the **Newton–Raphson method**, we found roots of equations using both the first and second derivatives.  
That approach works for simple, one-variable problems, but it has several drawbacks:

- ❌ Hard to extend to **functions with multiple parameters** (e.g., the Normal distribution with both mean \( \mu \) and variance \( \sigma^2 \))  
- ❌ Requires **second derivatives (Hessians)**, which are complex or expensive to compute  
- ❌ Can be **inefficient**, taking large jumps and overshooting the target  

To address these issues, we use **Gradient Descent** — a simpler, more general optimization method that scales to large models and high dimensions.

---

## 🌿 Concept and Intuition
Imagine you are standing on a **3D landscape** — a hill or valley representing your function’s shape.  
Each coordinate (\( \theta_1, \theta_2, ... \)) is a parameter in your model.

- The **gradient** tells you which direction is steepest uphill.  
- Moving **against** the gradient takes you downhill (toward the minimum).  
- Taking **small steps** repeatedly leads you to a local optimum.

The update rule is:

\[
\theta_{t+1} = \theta_t - \alpha \nabla f(\theta_t)
\]

where  
\( \alpha \) = step size (learning rate)  
\( \nabla f(\theta_t) \) = gradient (vector of first derivatives)

If we wanted to maximize the function, we would use:

\[
\theta_{t+1} = \theta_t + \alpha \nabla f(\theta_t)
\]

This is called **gradient ascent**, but in practice we usually minimize the **negative log-likelihood**, so the algorithm is known as **gradient descent**.

---

## 💫 Why “Descent”?
In statistics, we typically want to **maximize** the likelihood function \( \ell(\theta) \),  
but computers find it easier to **minimize** its negative counterpart:

\[
\max_\theta \ell(\theta) \quad \Leftrightarrow \quad \min_\theta [-\ell(\theta)]
\]

So even though conceptually we’re climbing “uphill” toward the MLE, in code we’re minimizing a function — hence the term *gradient descent*.

---

## 🧭 The Gradient Vector
For a function with multiple parameters \( f(\theta_1, \theta_2, ..., \theta_p) \),  
the gradient is a **vector of partial derivatives**:

\[
\nabla f(\theta) =
\begin{bmatrix}
\frac{\partial f}{\partial \theta_1} \\
\frac{\partial f}{\partial \theta_2} \\
\vdots \\
\frac{\partial f}{\partial \theta_p}
\end{bmatrix}
\]

It gives both the **direction** and **rate** of steepest increase.

So while Newton–Raphson uses tangent **lines** in one dimension,  
gradient descent uses tangent **planes** in higher dimensions.

---

## 🌸 Step Size (Learning Rate)
The **step size** \( \alpha \) (or learning rate) controls how far you move each iteration:

- 🚀 Too large → overshoot or diverge  
- 🐢 Too small → slow convergence  
- 🌿 Just right → smooth and stable descent

Choosing the right \( \alpha \) is crucial for convergence and efficiency.

---

## 🍀 Backtracking Line Search
To make step sizes adaptive, we use a **backtracking line search**.  

**Idea:**
1. Start with a large step size (t = 1).  
2. If the new step makes the function worse, shrink it: \( t = t \times b \), where \( b < 1 \) (e.g., 0.8).  
3. Repeat until improvement occurs, then accept that step.

This allows the algorithm to take **big steps when safe** and **small steps when necessary**.

---

### 💻 Example: Estimating μ and σ² for Normal Data

Here’s a demonstration using gradient descent to estimate the **mean** and **variance**  
of normally distributed height data, with a **backtracking line search**.

```{r}
# Gradient descent with backtracking line search
graddesc <- function(f, g, x0, b = 0.8, tol = 1e-2, max.iter = 40){
  i <- 0
  x <- x0
  results <- t(c(0, x, -f(x)))
  while (sqrt(g(x) %*% g(x)) > tol & i < max.iter) {
    i <- i + 1
    t <- 1
    while ((f(x + t * g(x)) < f(x)) & t > tol) {
      t <- t * b
    }
    x <- x + t * g(x)
    results <- rbind(results, t(c(i, x, -f(x))))
  }
  colnames(results) <- c('iter', paste0('x', 1:length(x)), 'f')
  return(results)
}

# Height data
heights <- c(170.1,161.6,175.2,166.3,165.6,165.8,152.0,155.8,168.6,154.3)

# Log-likelihood and gradient functions
height.ll <- function(x){
  -1 * length(heights) * log(2 * pi * x[2]) / 2 -
    sum((heights - x[1])^2) / (2 * x[2])
}

height.grad <- function(x){
  c(sum(heights - x[1]) / x[2],
    -1 * length(heights) / (2 * x[2]) +
      sum((heights - x[1])^2) / (2 * x[2]^2))
}

# Apply gradient descent
gd <- graddesc(f = height.ll, g = height.grad, x0 = c(150, 40))
gd[c(1:5, 41),]
```

## 🌈 The Central Limit Theorem (CLT)

### ☀️ The Sample Mean as a Random Variable
Every time we draw a sample from a population, the **sample mean** (\( \bar{X} \)) changes slightly.  
That means \( \bar{X} \) itself is a **random variable** with its own distribution.  

However, the distribution of the sample mean is *not* the same as the population’s distribution:
- If the population is **Uniform**, the sample mean is not uniform.  
- If the population is **Poisson**, the sample variance is not Poisson.  
- Even if the population is **skewed**, the sample mean’s distribution will smooth out.

---

### 🌿 What the CLT Says
The **Central Limit Theorem (CLT)** tells us how sample means behave when sample size increases.

Formally:

> Let \( X_1, X_2, ..., X_n \) be independent random variables from a population with  
> mean \( \mu \) and finite variance \( \sigma^2 \).  
> Then as \( n \to \infty \),
> \[
> Z = \frac{\bar{X} - \mu}{\sigma / \sqrt{n}} \to N(0,1)
> \]

That means:
\[
\bar{X} \sim N\left(\mu, \frac{\sigma^2}{n}\right)
\]

So, as the sample size grows:
- The **shape** of the sampling distribution of the mean becomes **Normal**,  
- The **spread** (Standard Error) becomes **smaller**: \( SE = \frac{\sigma}{\sqrt{n}} \).

---

### 💫 What This Means in Practice
Even if your population data are not Normal, the **sample means** will be approximately Normal when \( n \) is large.  
This allows us to:
- Estimate **confidence intervals**,  
- Perform **hypothesis tests**, and  
- Use **Normal-based methods** safely, even for non-Normal data.

---

### 📉 The Standard Error (SE)
The **CLT** tells us the shape of the sampling distribution, and the **Standard Error** gives its width:
\[
SE = \frac{\sigma}{\sqrt{n}}
\]

- The SE measures **how far the sample mean typically falls from the true mean**.  
- As \( n \) increases, the SE shrinks, meaning sample means become more stable.

| Concept | What it Describes | Formula | Purpose |
|:--|:--|:--|:--|
| **CLT** | Shape of sampling distribution of \( \bar{X} \) | \( N(\mu, \sigma^2/n) \) | Justifies using the Normal distribution |
| **SE** | Spread (standard deviation) of that distribution | \( \sigma / \sqrt{n} \) | Quantifies average distance from true mean |

---

## 🌷 Example — Heights of Ten Women

Let’s assume the same ten height measurements (cm) as before:

```{r}
heights <- c(170.1,161.6,175.2,166.3,165.6,165.8,152.0,155.8,168.6,154.3)
n <- length(heights)
sample_mean <- mean(heights)
pop_var <- 50
se <- sqrt(pop_var) / sqrt(n)

ci_lower <- sample_mean - 1.96 * se
ci_upper <- sample_mean + 1.96 * se
c("95% CI for mean height (cm)" = paste0(round(ci_lower,2), " to ", round(ci_upper,2)))
```

# 🌈 The Student’s t-Distribution

## ☀️ Motivation
The **Central Limit Theorem (CLT)** lets us approximate the sampling distribution of the sample mean using a Normal curve —  
but it assumes we already know the true population variance \( \sigma^2 \).

In practice, we almost never know \( \sigma^2 \).  
We must **estimate** it from the sample using \( s^2 \), which introduces extra uncertainty.  
This changes the behavior of our standardized statistic:

\[
\frac{\bar{X} - \mu}{s / \sqrt{n}}
\]

It no longer follows the Normal curve.  
Instead, it follows a slightly wider curve with thicker tails: the **Student’s t-distribution.**

---

```{r}
x <- seq(-4, 4, 0.01)
plot(x, dnorm(x), type = "l", lwd = 2, col = "#66CDAA",
     main = "Normal vs Student's t Distributions", ylab = "Density")
lines(x, dt(x, df = 5), col = "#F78FB3", lwd = 2)
lines(x, dt(x, df = 30), col = "#E75480", lwd = 2)
legend("topright", legend = c("Normal (z)", "t, df=5", "t, df=30"),
       col = c("#66CDAA", "#F78FB3", "#E75480"), lwd = 2, cex = 0.8)
```

---

## 🌿 The Problem in Words
When σ is known (theoretical case):

\[
Z = \frac{\bar{X} - \mu}{\sigma / \sqrt{n}} \sim N(0,1)
\]

When σ is *unknown* (real-world case):

\[
T = \frac{\bar{X} - \mu}{s / \sqrt{n}} \sim t_{n-1}
\]

The difference?  
We replaced the fixed population SD \( \sigma \) with the random sample SD \( s \).  
Because \( s \) varies across samples, the resulting distribution “spreads out” more, especially for small n.  
The t-distribution corrects for this.

---

## 📘 Variable Reference

| Symbol | Name | What It Represents |
|:--|:--|:--|
| \( X_i \) | Observation | A single data point in the sample |
| \( n \) | Sample size | Number of observations |
| \( \mu \) | True population mean | The real (unknown) mean of the population |
| \( \bar{X} \) | Sample mean | Average of the sample values |
| \( \sigma, \sigma^2 \) | True population SD / variance | Fixed but unknown spread of the population |
| \( s, s^2 \) | Sample SD / variance | Estimates of \( \sigma \) and \( \sigma^2 \) |
| \( SE \) | Standard Error | \( s / \sqrt{n} \); typical distance between sample and true mean |
| \( df \) | Degrees of freedom | \( n - 1 \); adjusts for estimating σ |
| \( Z \) | Z-statistic | Standardized value when σ known |
| \( T \) | T-statistic | Standardized value when σ unknown (uses s) |
| \( t_{n-1} \) | Student’s t-distribution | Probability model that \( T \) follows when σ unknown |
| \( t_{\alpha/2, n-1} \) | Critical t-value | Cutoff from t-distribution for desired confidence level |
| \( z_{\alpha/2} \) | Critical z-value | Cutoff from Normal distribution when σ known |
| \( \alpha \) | Significance level | e.g. 0.05 for 95% confidence |
| \( 1 - \alpha \) | Confidence level | Probability that interval contains μ |

---

## 💡 Extended Formula Reference (for CLT and t-tests)

| Formula | Meaning of Symbols |
|:--|:--|
| **CLT (σ known)**  \( Z = \frac{\bar{X} - \mu}{\sigma / \sqrt{n}} \) |  \( \bar{X} \): sample mean · \( \mu \): true mean · \( \sigma \): population SD · \( n \): sample size · \( Z \): standardized value |
| **t-statistic (σ unknown)**  \( T = \frac{\bar{X} - \mu}{s / \sqrt{n}} \) |  Same as above but uses \( s \) instead of \( \sigma \); follows t-distribution with \( df = n - 1 \) |
| **Confidence interval (t-based)**  \( \bar{X} \pm t_{\alpha/2, n-1}\frac{s}{\sqrt{n}} \) |  \( t_{\alpha/2, n-1} \): critical t-value · \( s/\sqrt{n} \): SE · bounds likely to contain μ |
| **Pooled variance (equal-var two-sample)**  \( s_p^2 = \frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2} \) |  Combines both groups’ variances; \( s_1, s_2 \): SDs · \( n_1, n_2 \): sample sizes |
| **Two-sample t-test (equal var)**  \( t = \frac{\bar{X}_1 - \bar{X}_2}{s_p\sqrt{1/n_1 + 1/n_2}} \) |  \( \bar{X}_1, \bar{X}_2 \): means · \( s_p \): pooled SD · df = \( n_1 + n_2 - 2 \) |
| **Welch (unequal var)**  \( t = \frac{\bar{X}_1 - \bar{X}_2}{\sqrt{s_1^2/n_1 + s_2^2/n_2}} \) |  Uses group variances directly; df ≈ \( \nu \) (Welch–Satterthwaite formula) |
| **One-sample proportion test**  \( z = \frac{\hat{p} - p_0}{\sqrt{p_0(1 - p_0)/n}} \) |  \( \hat{p} \): sample proportion · \( p_0 \): hypothesized proportion |

---

## 🌿 The t-Adjustment to the CLT
With an unknown variance, the CLT becomes:

\[
T = \frac{\bar{X} - \mu}{s / \sqrt{n}} \sim t_{n-1}
\]

As \( n \) grows, the t-distribution approaches the standard Normal.  
For small \( n \), its heavier tails account for the extra uncertainty in estimating \( \sigma \).

---

## 💫 Confidence Intervals

| Case | Formula | Description |
|:--|:--|:--|
| **σ known (z-based)** | \( \bar{X} \pm z_{\alpha/2}\frac{\sigma}{\sqrt{n}} \) | Uses Normal distribution |
| **σ unknown (t-based)** | \( \bar{X} \pm t_{\alpha/2, n-1}\frac{s}{\sqrt{n}} \) | Uses t-distribution (adjusts for estimated σ) |

---

## 💻 Example: One-Sample t-Test for Mean Height
```{r}
heights <- c(170.1,161.6,175.2,166.3,165.6,165.8,152.0,155.8,168.6,154.3)
n <- length(heights)
mean_h <- mean(heights)
sd_h <- sd(heights)
se_h <- sd_h / sqrt(n)

alpha <- 0.05
t_crit <- qt(1 - alpha/2, df = n - 1)
ci_lower <- mean_h - t_crit * se_h
ci_upper <- mean_h + t_crit * se_h

c("Sample mean" = round(mean_h, 2),
  "Sample SD" = round(sd_h, 2),
  "Standard Error" = round(se_h, 2),
  "95% CI Lower" = round(ci_lower, 2),
  "95% CI Upper" = round(ci_upper, 2))
```
## 🌍 Real-World Use Cases of the t-Distribution

The **Student’s t-distribution** appears constantly in applied data science, analytics, and experimental research —  
anytime we need to make inferences about a population mean without knowing the true variance.

Below are several examples showing when to use the t-distribution, what type of test applies,  
and *why* it is the correct choice.

---

## 🌿 Common Scenarios

| Scenario | Example | Appropriate Method | Why Use t-Distribution |
|:--|:--|:--|:--|
| 🧪 **Clinical trials** | Compare the mean blood pressure of a drug group vs. placebo | **Two-sample t-test** | Population variance unknown; small \( n \); data assumed Normal |
| ⚙️ **Quality control** | Compare mean product weights across two machines | **Pooled two-sample t-test** | Variances assumed equal; combines both groups’ variability |
| 💻 **Machine learning model validation** | Compare average model accuracies across folds | **Welch’s t-test** | Variances differ between folds; σ unknown |
| 📈 **A/B testing** | Compare mean click-through rates of two webpage designs | **Two-sample t-test or Welch’s test** | Proportion data approximated as Normal via CLT; σ estimated |
| 🛍️ **Marketing research** | Estimate the mean customer satisfaction score | **One-sample t-test** | σ unknown; single sample of ratings |
| 🧬 **Biological experiments** | Test average enzyme activity between treated and control groups | **Two-sample t-test** | Small sample, normally distributed measurements |
| 🏭 **Industrial processes** | Check if machine output matches a target mean | **One-sample t-test** | Compares sample mean to desired target value |
| 💡 **Survey analysis** | Estimate mean opinion score from respondents | **t-based confidence interval** | σ unknown; use \( s \) to estimate uncertainty |

---

## 🌸 Quick Guidelines

1. **Use the Normal (z) distribution** only if:
   - You *know* the population variance \( \sigma^2 \) (very rare), or  
   - Your sample is extremely large (\( n > 100 \)) so that \( s \) closely approximates \( \sigma \).

2. **Use the t-distribution** when:
   - The variance \( \sigma^2 \) is *unknown* (the usual case).  
   - The sample size is *small to moderate* (e.g., \( n < 30\!-\!50 \)).  
   - You are estimating or testing **means** (not proportions).

3. **Use Welch’s test** instead of the pooled version if:
   - The two samples have noticeably *different variances* or *unequal sample sizes.*

---

## ✨ Takeaway

> The **t-distribution** underpins most real-world inferential work.  
> It bridges the gap between small, uncertain samples and valid statistical inference —  
> letting us test hypotheses and build confidence intervals even when σ is unknown.

---

# 🌟 Handy Rules of Thumb

## ☀️ Overview
These are practical mental shortcuts that allow data scientists to:
- Approximate confidence intervals and sample sizes on the fly,  
- Sanity-check outputs,  
- Or give quick answers when detailed computation isn’t feasible.

They’re not rigorous — but they’re surprisingly accurate for quick reasoning.

---

## 🌿 Chebychev’s Inequality

\[
P(|X - \mu| < k\sigma) \geq 1 - \frac{1}{k^2}
\]

| Distance from Mean (kσ) | % Within Range (min.) | Example |
|:--|:--|:--|
| 1σ | ≥ 0% | No guarantee |
| 2σ | ≥ 75% | At least 3/4 of data within 2 SD |
| 3σ | ≥ 88.9% | At least 8/9 of data within 3 SD |
| 5σ | ≥ 96% | Very concentrated distributions |

✅ Works for *any* distribution with finite variance.  
⚠️ Conservative — true Normal data are usually tighter.

---

## 🌸 Common Normal Quantile Rule (68–95–99.7)

| Range | Approx. % | Description |
|:--|:--|:--|
| ±1σ | 68% | “1-sigma” rule |
| ±2σ | 95% | “2-sigma” rule |
| ±3σ | 99.7% | “3-sigma” rule |

> Many real-world distributions (large n Binomial, Poisson, t, χ²) approximately follow this pattern.

---

## 💫 Quick & Dirty Intervals and Tests

**Rule:**  
Under the null, a sample statistic will be within ±2 standard errors of the mean about 95% of the time.

Use this for:
- Rough hypothesis checks
- Quick margin-of-error or sample-size estimates

---

## 🧮 Example 1: Sample Size for Proportions

To estimate a proportion \( p \) within ±0.05 with 95% confidence:

\[
2 \times \sqrt{\frac{p(1-p)}{n}} = 0.05
\]

At max variance \( p = 0.5 \):
\[
n = 400
\]

| Desired Margin of Error | Approx. n |
|:--|:--|
| ±0.10 | 100 |
| ±0.05 | 400 |
| ±0.02 | 2500 |

---

### 📈 Example 2: Poisson CI
If λ = 100 births / 4 years → mean = 25 per year.

\[
SE = \sqrt{λ / years} = \sqrt{25} = 5
\]
95% CI ≈ \( 25 \pm 2(5) = [15, 35] \)

---

## 🧾 Example 3: χ² Quick Test
If χ² = 118 with df = 72:
\[
E(X) = 72,\ \ SD = \sqrt{2\times72}=12
\]
Expected 95% upper ≈ 72 + 2×12 = 96 → 118 > 96 → Reject H₀.

---

## 🌼 The Rule of Three

If you observe **zero** events out of *n* trials:

\[
p_{upper,95\%} \approx \frac{3}{n}
\]

| Observed Events | Sample Size (n) | 95% Upper Bound |
|:--|:--|:--|
| 0 | 100 | 3% |
| 0 | 300 | 1% |
| 0 | 1000 | 0.3% |

💡 Example:  
Inspect 300 products, find 0 defects → defect rate < 1% with 95% confidence.

---

## 🌷 Takeaway

> These rules aren’t exact, but they give you a powerful sense of scale:  
> - Chebychev’s Inequality — works for any data.  
> - 68–95–99.7 Rule — quick Normal approximation.  
> - ±2×SE — mental margin-of-error rule.  
> - Rule of Three — rapid upper bound for zero events.

They let you *think statistically* even when you don’t have code or software in front of you.
