---
title: "Study Notes: Chapter 1"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: cerulean   # try cerulean, journal, flatly, lumen, united, etc.
    highlight: tango  # syntax highlighting style
    toc: true         # adds a table of contents
    toc_float: true   # makes it float on the side
    df_print: kable   # nicer tables
---

```{r setup, include=FALSE}
# Load all packages you'll need
library(ggplot2)
library(dplyr)
library(tidyr)
```


# A Note on Notation

- **Observations (sample values):** lowercase Latin, e.g. `x‚ÇÅ, x‚ÇÇ, ‚Ä¶, x‚Çô`.
- **Random variables:** uppercase Latin, e.g. `X, Y`.
- **True parameters:** Greek letters, e.g. `Œº, œÉ¬≤, Œ∏`.
- **Estimates:** hats (\\hat{}) or matching lowercase, e.g. `ŒºÃÇ` or `xÃÑ`.
- **Vectors:** bold lowercase (\\mathbf{x}) or arrows (\\vec{x}).
- **Matrices:** bold uppercase (\\mathbf{X}).
- **Matrix elements:** double subscripts, e.g. `x_{ij}` = row *i*, column *j*.
- **Blackboard bold:** ‚Ñù (reals), ‚Ñ§ (integers), ùîº (expectation), ùïç (variance).

**Key connections:**
- $X$ = random variable, $x‚ÇÅ$ = one observed value, $Œº$ = true mean, $ŒºÃÇ$ = estimated mean.

---

# Preface: Inference

## Descriptive vs Inferential Statistics
- **Descriptive:** facts about the dataset (mean, median, proportions).
- **Inferential:** reasoning about the unseen generating process and population parameters.

Examples:
- Median of a dataset = descriptive.
- 95% confidence interval for population median = inferential.

---

## Core Process of Inference
1. Start with observed data (sample).
2. Assume data arise from a probability distribution.
3. Parameters (Œ∏) govern this distribution.
4. Use an **estimator** (recipe) to turn sample ‚Üí parameter guess.
5. Result = **estimate**.

**Estimator vs. Estimate:**
- Estimator = formula/function (the recipe).
- Estimate = value from your sample (the cookie batch).

---

## Spread, Variance, and Standard Deviation
- **Spread** = how scattered data are around the center (mean or median).
- **Variance (œÉ¬≤):** the **average squared distance** from the mean.
  - Units: squared (e.g. points¬≤).
  - Useful in formulas, regression, ANOVA, probability theory.
- **Standard deviation (œÉ):** the **square root of variance**, i.e. the **typical distance** from the mean.
  - Units: same as data (e.g. points).
  - Easier to interpret and communicate.

**Rule of thumb:**
- Use variance in **mathematical modeling and formulas**.
- Use standard deviation when **communicating spread** to others.

---

## Examples
- Meteor strikes: assume Poisson process with rate Œª. Estimate Œª from data, then predict probability of strikes in next 50 years.
- Survey: parameter = true population proportion $p$, estimate = sample proportion $pÃÇ$.
- Exam scores: variance = 25 ‚Üí standard deviation = 5. Interpretation: scores are typically about 5 points away from the mean.

---

## Parameters and Estimates Reference Table

| Parameter (Population, Greek) | Estimate (Sample, Latin/Hat) |
|-------------------------------|------------------------------|
| Mean (Œº)                      | Sample mean (ŒºÃÇ or xÃÑ)        |
| Variance (œÉ¬≤)                 | Sample variance (s¬≤)         |
| Standard deviation (œÉ)        | Sample standard deviation (s)|
| Proportion (p)                | Sample proportion (pÃÇ)        |
| Regression coefficients (Œ≤)   | Fitted coefficients (b)      |

---

## Statistical vs Machine Learning Models

### Statistical models
- **Assumption:** data are generated by a known probability distribution (Normal, Poisson, etc.).
- **Parameters:** small set (Œº, œÉ¬≤, Œ≤‚Äôs) that fully describe the model.
- **Goal:** estimate those parameters, then use them to explain or predict.
- **Strengths:**
  - Interpretable: parameters have clear meaning (e.g. Œ≤ = effect of X on Y).
  - Works with small datasets.
  - Computationally light (many solvable by hand).
- **Weaknesses:**
  - Risky if assumptions are wrong.
  - Can fail badly in rare-event regions.

### Machine learning models
- **Assumption:** no requirement of a single generating distribution.
- **Parameters:** exist (e.g. decision tree splits, neural net weights) but not tied to probability distributions.
- **Goal:** predictive accuracy and pattern discovery.
- **Strengths:**
  - Very flexible, handles messy/big/heterogeneous data.
  - Often higher predictive accuracy.
- **Weaknesses:**
  - Less interpretable (harder to explain why it works).
  - Data-hungry and computationally expensive.

### Consultant‚Äôs framing
- **Statistical models** ‚Üí best for *explaining how the world works*.
- **Machine learning models** ‚Üí best for *predicting what‚Äôs likely to happen next*.

---

# Univariate Likelihood

## Definition
- **Likelihood function**: a score of how well a parameter value ($Œ∏$) explains observed data.
- General form (for $n$ independent observations $x_1, ‚Ä¶, x_n$):
  - **Discrete case:**  
    $L(Œ∏) = \\prod_{i=1}^n f(x_i ; Œ∏)$, where $f(x_i ; Œ∏)$ is the probability of $x_i$ under parameter $Œ∏$.
  - **Continuous case:**  
    $L(Œ∏) = \\prod_{i=1}^n f(x_i ; Œ∏)$, where $f$ is a PDF (density). 

## How to read the formula
- **n** = number of observations in the dataset.
- **$x_i$** = the $i$-th observed value.
- **$Œ∏$ (theta)** = unknown parameter(s) of the distribution (e.g. p for Bernoulli, Œº and œÉ¬≤ for Normal).
- **$f(x_i ; Œ∏)$** = probability (discrete) or density (continuous) of seeing $x_i$ given $Œ∏$.
- **Likelihood** = multiply them all together ‚Üí ‚Äúhow compatible is this $Œ∏$ with the entire dataset?‚Äù

## Discrete vs Continuous Likelihood
- **Discrete:** probabilities of exact outcomes, each between 0 and 1, and they sum to 1 across all possible outcomes.
  - Example: coin flips (Bernoulli).
  - Likelihood multiplies probabilities for observed outcomes.
- **Continuous:** exact values have probability 0, so we use a **probability density function (PDF)**.
  - Densities can be greater than 1, but areas under the curve sum to 1.
  - Likelihood multiplies densities at observed values.

**Analogy:**
- Discrete = counting marbles (probability of each marble is exact).
- Continuous = measuring water height in a container (density tells how likely values are to fall nearby).

## Examples
- **Coin flips:** 50 flips, 27 heads.  Bernoulli distribution with parameter $p$.
  - Likelihood: $L(p) = p^{27}(1-p)^{23}$.
- **Normal data:** heights from 10 women.  Normal distribution with parameters $(Œº,œÉ¬≤)$.
  - Likelihood: $L(Œº,œÉ¬≤) = \\prod f(x_i; Œº,œÉ¬≤)$. 

**Key idea:** In inference we flip the usual question. Instead of ‚Äúgiven parameter ‚Üí probability of data,‚Äù we ask ‚Äúgiven data ‚Üí which parameter values make it most likely?‚Äù

---

## Log-likelihood
- **Definition:** The logarithm of the likelihood function.
- Formula: $\\ell(Œ∏) = \\log L(Œ∏) = \\sum_{i=1}^n \\log f(x_i ; Œ∏)$.

### Variable meanings
- **n** = number of observations.
- **$x_i$** = the $i$-th observed value.
- **$Œ∏$ (theta)** = unknown parameter(s) of the distribution.
- **$f(x_i ; Œ∏)$** = probability (discrete) or density (continuous) of $x_i$ given $Œ∏$.
- **$L(Œ∏)$** = likelihood function.
- **$\\ell(Œ∏)$** = log-likelihood function.

### Why use log-likelihood?
1. Products become sums: easier to handle large $n$.
2. Maximization is preserved (log is monotonic).
3. Improves numerical stability (avoids tiny products underflowing to 0).

### Example: Coin flips (27 heads, 23 tails)
- Likelihood: $L(p) = p^{27}(1-p)^{23}$.
- Log-likelihood: $\\ell(p) = 27 \\log(p) + 23 \\log(1-p)$.
- Same maximum, but much easier to maximize (sums instead of products).

**Key idea:** Likelihood and log-likelihood contain the same information. Log-likelihood is simply more convenient for differentiation and optimization.

---

## Derivative Trick
- To maximize likelihood, differentiate the log-likelihood with respect to the parameter(s) and set the derivative equal to zero.
- Critical points (where slope = 0) give the maximum likelihood estimator (MLE).

### Examples
- **Coin flips (Bernoulli):**
  - Likelihood: $L(p) = p^{27}(1-p)^{23}$.
  - Log-likelihood: $\\ell(p) = 27\\log(p) + 23\\log(1-p)$.
  - Derivative: $\\ell'(p) = \\frac{27}{p} - \\frac{23}{1-p}$.
  - Solution: $pÃÇ = 27/50 = 0.54$ (the sample proportion).

- **Heights (Normal):**
  - MLE for mean $Œº$ = sample mean $xÃÑ$.
  - MLE for variance $œÉ¬≤$ = uncorrected sample variance (slightly biased).

### Interpretation
- MLE gives the parameter values that make observed data look the **least unusual**.
- As sample size increases, likelihood becomes sharper and estimates converge toward the true parameter (consistency of MLE).

---

## Visualizing Likelihood and Log-likelihood: Coin Flips

```{r}
#library(dplyr)
library(tidyr)
library(ggplot2)

# Parameters
n_heads <- 27
n_tails <- 23

# Likelihood functions
likelihood <- function(p) p^n_heads * (1 - p)^n_tails
loglik     <- function(p) n_heads * log(p) + n_tails * log(1 - p)
dloglik    <- function(p) n_heads / p - n_tails / (1 - p)

# Sequence of p values
p <- seq(0.01, 0.99, length.out = 200)

# Data frame of values
df <- tibble(
  p = p,
  Likelihood = likelihood(p),
  LogLikelihood = loglik(p),
  Derivative = dloglik(p)
)

# Reshape for plotting
df_long <- df %>%
  pivot_longer(-p, names_to = "Function", values_to = "Value")

# Plot
ggplot(df_long, aes(x = p, y = Value, color = Function)) +
  geom_line(linewidth = 1.2) +
  facet_wrap(~ Function, scales = "free_y", ncol = 1) +
  geom_vline(xintercept = n_heads / (n_heads + n_tails),
             linetype = "dashed", color = "hotpink", linewidth = 1) +
  theme_minimal(base_size = 14) +
  theme(
    strip.text = element_text(face = "bold", size = 16, color = "purple"),
    panel.grid.minor = element_blank(),
    plot.background = element_rect(fill = "#fff8f8", color = NA),
    panel.background = element_rect(fill = "#fff8f8", color = NA),
    plot.title = element_text(hjust = 0.5, size = 18, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 14, color = "gray30")
  ) +
  labs(
    title = "Likelihood, Log-likelihood, and Derivative (Coin Flips)",
    subtitle = "Dashed line shows the MLE at p = 27/50",
    x = "p (probability of heads)",
    y = "Value"
  )

```

---

# Hypothesis Testing

## Core Ideas
- **Null hypothesis (H‚ÇÄ):** the default assumption (e.g., coin is fair, ads don‚Äôt work).
- **Alternative hypothesis (H‚ÇÅ):** the competing idea (e.g., coin is biased, ads improve conversions).
  - **Two-tailed:** you only know it‚Äôs different (‚â†).
  - **One-tailed:** you suspect a direction (> or <).
- **Test statistic (T):** a summary of the data (e.g., number of heads, difference in means).
  - Must follow a known distribution under H‚ÇÄ (binomial, normal, etc.).
- **p-value:** probability of observing data *at least as extreme* as yours if H‚ÇÄ is true.
  - Small p-value ‚Üí data are unlikely under H‚ÇÄ ‚Üí evidence against H‚ÇÄ.
- **Significance threshold (Œ±):** cutoff chosen in advance (commonly 0.05).
  - If p ‚â§ Œ± ‚Üí reject H‚ÇÄ (evidence for effect).
  - If p > Œ± ‚Üí fail to reject H‚ÇÄ (not enough evidence).

---

## Example: Coin Flips
- H‚ÇÄ: coin is fair (p = 0.5).  
- H‚ÇÅ: coin is biased (p ‚â† 0.5).  
- Test statistic: number of heads in 50 flips.  
- Observed: 27 heads.  
- p-value: probability of 27 or more heads OR 23 or fewer heads under a fair coin.  
- If p ‚â§ 0.05, we‚Äôd reject H‚ÇÄ. Otherwise, data are consistent with a fair coin.

---

## Example: A/B Testing Ads
- H‚ÇÄ: Ad does **not** increase conversion rate.  
- H‚ÇÅ: Ad increases conversion rate.  
- Suppose:
  - Control group conversion = 5%.  
  - Ad group conversion = 7%.  
  - Difference = 2%.  
- Compute test statistic (difference in proportions).  
- Find p-value = probability of a difference ‚â• 2% under H‚ÇÄ.  
- Compare with Œ± (e.g., 0.05).  
- If p ‚â§ Œ± ‚Üí evidence that the ad works.  

---

# Estimator Bias and Variance

Not all estimators are equally good. Some are biased, some have high variance, and some balance the two better.  

## Definitions

- **Bias** of an estimator:  
  $ \text{Bias}(\hat{\theta}) = E[\hat{\theta}] - \theta $  
  - Measures whether the estimator systematically over- or underestimates the true parameter $ \theta $.  
  - If $E[\hat{\theta}] = \theta$ for all possible samples ‚Üí estimator is **unbiased**.  

- **Variance** of an estimator:  
  $ \text{Var}(\hat{\theta}) = E\big[(\hat{\theta} - E[\hat{\theta}])^2\big] $  
  - Measures how much the estimator varies across repeated samples.  

- **Standard error (SE):**  
  $ SE(\hat{\theta}) = \sqrt{\text{Var}(\hat{\theta})} $  
  - Standard deviation of the estimator ‚Üí ‚Äúhow much $\hat{\theta}$ wiggles around‚Äù in repeated sampling.  

---

## Archery Analogy üéØ

Think of different estimators as archers aiming at the bullseye (the true parameter, $ \theta $):  

- **High bias, low variance:** shots are clustered together but consistently off-target.  
- **Low bias, high variance:** shots spread widely around the target, but average to the center.  
- **Low bias, low variance:** shots cluster tightly around the bullseye (ideal).  
- **High bias, high variance:** shots are both scattered and off-target (worst).  

---

## Bias-Variance Tradeoff

- **Mean Squared Error (MSE):**  
  $ MSE(\hat{\theta}) = \text{Bias}(\hat{\theta})^2 + \text{Var}(\hat{\theta}) $  

- Between two estimators with the same MSE, one may have higher bias but lower variance, or vice versa.  
- This **bias-variance tradeoff** appears in both statistics and machine learning.  

---

## Example

Suppose we want to estimate the mean height of a population ($ \mu $):  

- Estimator A: sample mean ($ \bar{x} $).  
  - Unbiased, low variance (gets better as $n$ increases).  

- Estimator B: average of sample min and max.  
  - Biased (tends to over/underestimate), higher variance.  

Simulation shows estimator A consistently outperforms estimator B.  

---

## Visualizing Bias and Variance (Archery Analogy)

```{r fig.width=6, fig.height=6, message=FALSE, warning=FALSE}
library(ggplot2)

# Create fake "shots" for different scenarios
set.seed(123)
n <- 30
true_mu <- c(0, 0)  # bullseye

scenarios <- data.frame(
  x = c(rnorm(n, 2, 0.5),   # High bias, low variance
        rnorm(n, 0, 2),     # Low bias, high variance
        rnorm(n, 0, 0.5),   # Low bias, low variance
        rnorm(n, 2, 2)),    # High bias, high variance
  y = c(rnorm(n, 2, 0.5),
        rnorm(n, 0, 2),
        rnorm(n, 0, 0.5),
        rnorm(n, 2, 2)),
  scenario = rep(c("High Bias, Low Var",
                   "Low Bias, High Var",
                   "Low Bias, Low Var",
                   "High Bias, High Var"), each = n)
)

ggplot(scenarios, aes(x, y)) +
  geom_point(color = "tomato", alpha = 0.7) +
  geom_point(aes(x = true_mu[1], y = true_mu[2]),
             color = "blue", size = 5, shape = 4, stroke = 2) +
  facet_wrap(~ scenario) +
  coord_equal() +
  theme_minimal(base_size = 14) +
  labs(title = "Bias vs Variance: Archery Analogy üéØ",
       subtitle = "Blue cross = true parameter (Œº). Red dots = estimator outcomes.",
       x = "", y = "")
```

# Confidence Intervals

## Motivation
- A **confidence interval (CI)** is a range of reasonable guesses for an unknown parameter.
- Wider CIs ‚Üí more confidence of covering Œº, but less precise.
- Narrower CIs ‚Üí more precision, but higher risk of missing Œº.
- You decide the **confidence level** (commonly 90%, 95%, 99%).

---

## Formula (Normal case, known œÉ)
For a population mean Œº with sample mean $\bar{x}$:

\[
CI = \bar{x} \; \pm \; z_{\alpha/2} \cdot SE
\]

Where:
- $\bar{x}$ = sample mean  
- $SE = \frac{\sigma}{\sqrt{n}}$ = standard error of the mean  
- $z_{\alpha/2}$ = critical value from the standard normal distribution  
- $\alpha = 1 - \text{confidence level}$ (e.g., 0.05 for 95%)  

---

## Understanding $z_{\alpha/2}$
- For a **95% CI**: $\alpha = 0.05$, so each tail = 0.025.  
- Look up in Z-table (or compute): $z_{0.025} = 1.96$.  
- Meaning: 95% of the normal curve lies between -1.96 and +1.96.  

**Other common values:**
- 90% CI ‚Üí $z_{0.05} = 1.645$  
- 95% CI ‚Üí $z_{0.025} = 1.96$  
- 99% CI ‚Üí $z_{0.005} = 2.576$  

---

## Example: U.S. Women‚Äôs Heights
- Sample size: $n = 1000$  
- Sample mean: $\bar{x} = 65$ in (5'5")  
- Population SD: $\sigma = 3$ in  
- Standard error: $SE = \frac{3}{\sqrt{1000}} \approx 0.095$  
- 95% CI: $65 \pm 1.96 \cdot 0.095 = (64.81, 65.19)$  

Interpretation: We are 95% confident that the **true average height of U.S. women (18‚Äì50)** is between **64.8 and 65.2 inches**.

---

## Common Misconceptions
‚ùå ‚Äú95% of data values are inside the CI.‚Äù (No ‚Üí we are estimating the parameter, not bounding the dataset.)  
‚ùå ‚ÄúThere‚Äôs a 95% chance Œº is inside the CI.‚Äù (No ‚Üí Œº is fixed; the *method* succeeds 95% of the time.)  
‚úÖ Correct: ‚ÄúIf we repeated this process many times, 95% of intervals would contain Œº.‚Äù  

---

## Visualization

```{r fig.width=7, fig.height=4, message=FALSE, warning=FALSE}
library(ggplot2)

# Data for normal curve
x <- seq(-4, 4, length.out = 1000)
y <- dnorm(x)

df <- data.frame(x, y)

# Confidence level
z <- 1.96

ggplot(df, aes(x, y)) +
  geom_line(color = "darkblue", linewidth = 1.2) +
  geom_area(data = subset(df, x >= -z & x <= z),
            aes(x, y), fill = "skyblue", alpha = 0.6) +
  geom_vline(xintercept = c(-z, z), linetype = "dashed",
             color = "hotpink", linewidth = 1) +
  annotate("text", x = 0, y = 0.05, label = "95% Confidence Area",
           size = 5, color = "darkblue", fontface = "bold") +
  annotate("text", x = -z, y = 0.02, label = "-1.96",
           size = 4, color = "hotpink", vjust = -1) +
  annotate("text", x = z, y = 0.02, label = "+1.96",
           size = 4, color = "hotpink", vjust = -1) +
  theme_minimal(base_size = 14) +
  labs(title = "Confidence Interval on Normal Curve",
       subtitle = "Shaded area = 95% middle region")
```
# Identifying Distributions

## Motivation
So far we assumed the data‚Äôs distribution (Normal, Binomial, Poisson, etc.) was known.  
In reality, data rarely comes labeled with a distribution. Our job is to guess a reasonable one.

We‚Äôll use the **E-commerce dataset example**:  

- **Spend per order** ‚Üí continuous, skewed ‚Üí candidate: Lognormal or Gamma.  
- **Orders per customer per month** ‚Üí counts ‚Üí candidate: Poisson or Negative Binomial.  
- **Time between purchases** ‚Üí waiting times ‚Üí candidate: Exponential.  
- **Ratings (1‚Äì5 stars)** ‚Üí bounded, discrete ‚Üí candidate: Categorical.  

---

## Step 1: Checklist Questions
1. Is the variable **discrete or continuous**?  
2. Is it **bounded** (e.g. ratings 1‚Äì5)?  
3. Is it **always positive** (e.g. spend, orders)?  
4. Does it cover **many magnitudes** (log scale helpful)?  

---

## Step 2: Empirical Distribution Function (ECDF)

An **ECDF** is like a ‚Äúsample-based CDF.‚Äù  
It plots the proportion of values ‚â§ a threshold.

```{r fig.width=6, fig.height=5}
set.seed(123)

# Simulate customer spend (lognormal, skewed)
spend <- rlnorm(1000, meanlog = 4, sdlog = 0.5)

# ECDF of sample
library(ggplot2)
ggplot(data.frame(spend), aes(spend)) +
  stat_ecdf(geom="step", color="hotpink", linewidth=1.2) +
  stat_function(fun = plnorm, args = list(meanlog=4, sdlog=0.5),
                color="skyblue", linewidth=1) +
  theme_minimal(base_size = 14) +
  labs(title="ECDF vs Theoretical Lognormal CDF",
       x="Spend per Order ($)",
       y="Cumulative Probability",
       subtitle="Hotpink = Sample ECDF | Blue = Lognormal CDF")
```

# Coding it up in R

## 1. Automated MLE with `fitdistr`
In simple cases (normal, exponential, Poisson, etc.), we can let R compute maximum likelihood estimates directly.  

Example: **Customer purchase waiting times** modeled as exponential.  

```{r}
library(MASS)
set.seed(1138)

# Simulate: times between purchases (true Œª=5)
expsample <- rexp(n=5000, rate=5)

# Automatic solution
auto.sol <- fitdistr(x=expsample, densfun="exponential")

# Manual MLE (ŒªÃÇ = 1 / mean)
man.sol <- 1 / mean(expsample)

c(auto.solution=auto.sol$estimate,
  manual.solution=man.sol)
```

## Visualizing the Log-likelihood
We can plot how the log-likelihood changes across candidate values of Œª.
```{r}
xticks <- seq(1, 9, 0.01)
loglik <- sapply(xticks, function(z) sum(dexp(expsample, z, log=TRUE)))

plot(xticks, loglik, type="l",
     xlab="Œª (rate parameter)",
     ylab="Log-likelihood",
     col="purple", lwd=2)
abline(v=c(man.sol, 5), col=c("black","blue"), lty=2, lwd=2)
text(x=c(man.sol, 5), y=max(loglik)-100,
     labels=c("MLE","Truth"),
     col=c("black","blue"), pos=c(2,4))
```

## Custom Likelihoods with optim()
Sometimes we need to build our own likelihood.
Example: Truncated Poisson ‚Äî say we only observe customers who make ‚â•1 purchase (no zeroes in data).
```{r}
set.seed(1139)
hidden.data <- rpois(n=50000, lambda=0.07)
observed.data <- hidden.data[hidden.data != 0]

# Custom truncated Poisson log-likelihood
ll.trunc.pois <- function(lambda, x){
  sum(x)*log(lambda) - lambda*sum(1/factorial(x)) -
    length(x)*log(1 - exp(-lambda))
}

# Optimize using MLE
opt.sol <- optim(par=0.1, fn=ll.trunc.pois,
                 x=observed.data,
                 control=list(fnscale=-1),
                 method="L-BFGS-B",
                 lower=0.01, upper=0.5)

opt.sol$par
```

## Hypothesis Testing in R
Scenario 1: Two-sided binomial test

Coin flips (fair coin?)
```{r}
p.value <- 2*pbinom(q=4933, size=10000, prob=0.5)
p.value
```

Scenario 2: One-sided exponential test

Bus arrivals ‚Äî claim: mean wait = 12 min. You observe 18 and 30.

```{r}
p.value <- (1 - pexp(18/12)) * (1 - pexp(30/12))
p.value
```
Reject H‚ÇÄ, buses arrive less often than claimed.

Scenario 3: One-sided permutation test
Tea experiment (Muriel Bristol guessing cups).
```{r}
p.value <- 1/choose(8,4)
p.value
```
Reject H‚ÇÄ, she could taste the difference.

## Exact Confidence Intervals
Example: Voter survey

1000 people ‚Üí 65 say ‚Äúthird party.‚Äù

```{r}
binom.test(x=65, n=1000, conf.level=0.90)
```

Example: Penny stocks

500 penny stocks, 89 go bankrupt.
One-sided 80% upper bound:

```{r}
binom.test(x=89, n=500, alternative="less", conf.level=0.80)
```
Result: true bankruptcy rate ‚â§ 19.4% (with 80% confidence).

## Identifying Distributions with K-S Tests
One-sample test (dog food weights)

```{r}
x <- c(25.3, 25.1, 25.1, 25.5, 25.2,
       24.7, 24.8, 24.9, 24.8, 25.5)

ks.test(x, pnorm, mean=25.6, sd=0.3)
```
Reject H‚ÇÄ. New bags ‚â† Normal(25.6, 0.3¬≤).

Two-sample test

```{r}
y <- c(25.4, 25.7, 26.2, 25.8, 25.6,
       26.4, 25.4, 24.9, 26.0, 26.0)

ks.test(x,y)
```
New process differs from old sample.

Unknown distribution (EDA)
Fit both Normal and Cauchy to sample:

```{r}
(cauchy.pars <- fitdistr(y, dcauchy,
                        start=list(location=25,scale=0.5))$estimate)
(normal.pars <- fitdistr(y, dnorm,
                        start=list(mean=25,sd=0.5))$estimate)
```
Both yield large p-values in K-S ‚Üí either distribution plausible.
More data would help distinguish.