---
title: "Study Notes: Chapter 1"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: cerulean   # try cerulean, journal, flatly, lumen, united, etc.
    highlight: tango  # syntax highlighting style
    toc: true         # adds a table of contents
    toc_float: true   # makes it float on the side
    df_print: kable   # nicer tables
---

# A Note on Notation

- **Observations (sample values):** lowercase Latin, e.g. `x‚ÇÅ, x‚ÇÇ, ‚Ä¶, x‚Çô`.
- **Random variables:** uppercase Latin, e.g. `X, Y`.
- **True parameters:** Greek letters, e.g. `Œº, œÉ¬≤, Œ∏`.
- **Estimates:** hats (\\hat{}) or matching lowercase, e.g. `ŒºÃÇ` or `xÃÑ`.
- **Vectors:** bold lowercase (\\mathbf{x}) or arrows (\\vec{x}).
- **Matrices:** bold uppercase (\\mathbf{X}).
- **Matrix elements:** double subscripts, e.g. `x_{ij}` = row *i*, column *j*.
- **Blackboard bold:** ‚Ñù (reals), ‚Ñ§ (integers), ùîº (expectation), ùïç (variance).

**Key connections:**
- $X$ = random variable, $x‚ÇÅ$ = one observed value, $Œº$ = true mean, $ŒºÃÇ$ = estimated mean.

---

# Preface: Inference

## Descriptive vs Inferential Statistics
- **Descriptive:** facts about the dataset (mean, median, proportions).
- **Inferential:** reasoning about the unseen generating process and population parameters.

Examples:
- Median of a dataset = descriptive.
- 95% confidence interval for population median = inferential.

---

## Core Process of Inference
1. Start with observed data (sample).
2. Assume data arise from a probability distribution.
3. Parameters (Œ∏) govern this distribution.
4. Use an **estimator** (recipe) to turn sample ‚Üí parameter guess.
5. Result = **estimate**.

**Estimator vs. Estimate:**
- Estimator = formula/function (the recipe).
- Estimate = value from your sample (the cookie batch).

---

## Spread, Variance, and Standard Deviation
- **Spread** = how scattered data are around the center (mean or median).
- **Variance (œÉ¬≤):** the **average squared distance** from the mean.
  - Units: squared (e.g. points¬≤).
  - Useful in formulas, regression, ANOVA, probability theory.
- **Standard deviation (œÉ):** the **square root of variance**, i.e. the **typical distance** from the mean.
  - Units: same as data (e.g. points).
  - Easier to interpret and communicate.

**Rule of thumb:**
- Use variance in **mathematical modeling and formulas**.
- Use standard deviation when **communicating spread** to others.

---

## Examples
- Meteor strikes: assume Poisson process with rate Œª. Estimate Œª from data, then predict probability of strikes in next 50 years.
- Survey: parameter = true population proportion $p$, estimate = sample proportion $pÃÇ$.
- Exam scores: variance = 25 ‚Üí standard deviation = 5. Interpretation: scores are typically about 5 points away from the mean.

---

## Parameters and Estimates Reference Table

| Parameter (Population, Greek) | Estimate (Sample, Latin/Hat) |
|-------------------------------|------------------------------|
| Mean (Œº)                      | Sample mean (ŒºÃÇ or xÃÑ)        |
| Variance (œÉ¬≤)                 | Sample variance (s¬≤)         |
| Standard deviation (œÉ)        | Sample standard deviation (s)|
| Proportion (p)                | Sample proportion (pÃÇ)        |
| Regression coefficients (Œ≤)   | Fitted coefficients (b)      |

---

## Statistical vs Machine Learning Models

### Statistical models
- **Assumption:** data are generated by a known probability distribution (Normal, Poisson, etc.).
- **Parameters:** small set (Œº, œÉ¬≤, Œ≤‚Äôs) that fully describe the model.
- **Goal:** estimate those parameters, then use them to explain or predict.
- **Strengths:**
  - Interpretable: parameters have clear meaning (e.g. Œ≤ = effect of X on Y).
  - Works with small datasets.
  - Computationally light (many solvable by hand).
- **Weaknesses:**
  - Risky if assumptions are wrong.
  - Can fail badly in rare-event regions.

### Machine learning models
- **Assumption:** no requirement of a single generating distribution.
- **Parameters:** exist (e.g. decision tree splits, neural net weights) but not tied to probability distributions.
- **Goal:** predictive accuracy and pattern discovery.
- **Strengths:**
  - Very flexible, handles messy/big/heterogeneous data.
  - Often higher predictive accuracy.
- **Weaknesses:**
  - Less interpretable (harder to explain why it works).
  - Data-hungry and computationally expensive.

### Consultant‚Äôs framing
- **Statistical models** ‚Üí best for *explaining how the world works*.
- **Machine learning models** ‚Üí best for *predicting what‚Äôs likely to happen next*.

---

# Univariate Likelihood

## Definition
- **Likelihood function**: a score of how well a parameter value ($Œ∏$) explains observed data.
- General form (for $n$ independent observations $x_1, ‚Ä¶, x_n$):
  - **Discrete case:**  
    $L(Œ∏) = \\prod_{i=1}^n f(x_i ; Œ∏)$, where $f(x_i ; Œ∏)$ is the probability of $x_i$ under parameter $Œ∏$.
  - **Continuous case:**  
    $L(Œ∏) = \\prod_{i=1}^n f(x_i ; Œ∏)$, where $f$ is a PDF (density). 

## How to read the formula
- **n** = number of observations in the dataset.
- **$x_i$** = the $i$-th observed value.
- **$Œ∏$ (theta)** = unknown parameter(s) of the distribution (e.g. p for Bernoulli, Œº and œÉ¬≤ for Normal).
- **$f(x_i ; Œ∏)$** = probability (discrete) or density (continuous) of seeing $x_i$ given $Œ∏$.
- **Likelihood** = multiply them all together ‚Üí ‚Äúhow compatible is this $Œ∏$ with the entire dataset?‚Äù

## Discrete vs Continuous Likelihood
- **Discrete:** probabilities of exact outcomes, each between 0 and 1, and they sum to 1 across all possible outcomes.
  - Example: coin flips (Bernoulli).
  - Likelihood multiplies probabilities for observed outcomes.
- **Continuous:** exact values have probability 0, so we use a **probability density function (PDF)**.
  - Densities can be greater than 1, but areas under the curve sum to 1.
  - Likelihood multiplies densities at observed values.

**Analogy:**
- Discrete = counting marbles (probability of each marble is exact).
- Continuous = measuring water height in a container (density tells how likely values are to fall nearby).

## Examples
- **Coin flips:** 50 flips, 27 heads.  Bernoulli distribution with parameter $p$.
  - Likelihood: $L(p) = p^{27}(1-p)^{23}$.
- **Normal data:** heights from 10 women.  Normal distribution with parameters $(Œº,œÉ¬≤)$.
  - Likelihood: $L(Œº,œÉ¬≤) = \\prod f(x_i; Œº,œÉ¬≤)$. 

**Key idea:** In inference we flip the usual question. Instead of ‚Äúgiven parameter ‚Üí probability of data,‚Äù we ask ‚Äúgiven data ‚Üí which parameter values make it most likely?‚Äù

---

## Log-likelihood
- **Definition:** The logarithm of the likelihood function.
- Formula: $\\ell(Œ∏) = \\log L(Œ∏) = \\sum_{i=1}^n \\log f(x_i ; Œ∏)$.

### Variable meanings
- **n** = number of observations.
- **$x_i$** = the $i$-th observed value.
- **$Œ∏$ (theta)** = unknown parameter(s) of the distribution.
- **$f(x_i ; Œ∏)$** = probability (discrete) or density (continuous) of $x_i$ given $Œ∏$.
- **$L(Œ∏)$** = likelihood function.
- **$\\ell(Œ∏)$** = log-likelihood function.

### Why use log-likelihood?
1. Products become sums: easier to handle large $n$.
2. Maximization is preserved (log is monotonic).
3. Improves numerical stability (avoids tiny products underflowing to 0).

### Example: Coin flips (27 heads, 23 tails)
- Likelihood: $L(p) = p^{27}(1-p)^{23}$.
- Log-likelihood: $\\ell(p) = 27 \\log(p) + 23 \\log(1-p)$.
- Same maximum, but much easier to maximize (sums instead of products).

**Key idea:** Likelihood and log-likelihood contain the same information. Log-likelihood is simply more convenient for differentiation and optimization.

---

## Derivative Trick
- To maximize likelihood, differentiate the log-likelihood with respect to the parameter(s) and set the derivative equal to zero.
- Critical points (where slope = 0) give the maximum likelihood estimator (MLE).

### Examples
- **Coin flips (Bernoulli):**
  - Likelihood: $L(p) = p^{27}(1-p)^{23}$.
  - Log-likelihood: $\\ell(p) = 27\\log(p) + 23\\log(1-p)$.
  - Derivative: $\\ell'(p) = \\frac{27}{p} - \\frac{23}{1-p}$.
  - Solution: $pÃÇ = 27/50 = 0.54$ (the sample proportion).

- **Heights (Normal):**
  - MLE for mean $Œº$ = sample mean $xÃÑ$.
  - MLE for variance $œÉ¬≤$ = uncorrected sample variance (slightly biased).

### Interpretation
- MLE gives the parameter values that make observed data look the **least unusual**.
- As sample size increases, likelihood becomes sharper and estimates converge toward the true parameter (consistency of MLE).

---

## Visualizing Likelihood and Log-likelihood: Coin Flips

```{r}
library(ggplot2)
library(dplyr)
library(tidyr)

n_heads <- 27
n_tails <- 23

# Likelihood functions
likelihood <- function(p) p^n_heads * (1 - p)^n_tails
loglik     <- function(p) n_heads * log(p) + n_tails * log(1 - p)
dloglik    <- function(p) n_heads / p - n_tails / (1 - p)

p <- seq(0.01, 0.99, length.out = 200)

df <- tibble(
  p = p,
  Likelihood = likelihood(p),
  LogLikelihood = loglik(p),
  Derivative = dloglik(p)
)

# Reshape for plotting
df_long <- df %>%
  pivot_longer(-p, names_to = "Function", values_to = "Value")

# Pretty ggplot
ggplot(df_long, aes(x = p, y = Value, color = Function)) +
  geom_line(linewidth = 1.2) +
  facet_wrap(~ Function, scales = "free_y", ncol = 1) +
  geom_vline(xintercept = n_heads / (n_heads + n_tails),
             linetype = "dashed", color = "hotpink", linewidth = 1) +
  theme_minimal(base_size = 14) +
  theme(
    strip.text = element_text(face = "bold", size = 16, color = "purple"),
    panel.grid.minor = element_blank(),
    plot.background = element_rect(fill = "#fff8f8", color = NA),
    panel.background = element_rect(fill = "#fff8f8", color = NA),
    plot.title = element_text(hjust = 0.5, size = 18, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 14, color = "gray30")
  ) +
  labs(
    title = "Likelihood, Log-likelihood, and Derivative (Coin Flips)",
    subtitle = "Dashed line shows the MLE at p = 27/50",
    x = "p (probability of heads)",
    y = "Value"
  )


